# âœ… å‰ 2 å‘¨å¾…åŠæ¸…å•ï¼ˆå¯å‹¾é€‰ï¼‰

> èŠ‚å¥å»ºè®®ï¼šæ¯å‘¨ 5 å°æ—¶å·¦å³ï¼ˆå¯æ‹†æˆ 3Ã—90minï¼‰ã€‚å®Œæˆåº¦ä»¥â€œè„šæ‰‹æ¶å¯è·‘é€š + æœ‰æœ€å°äº§å‡ºâ€ä¸ºå‡†ã€‚

## Week 1ï¼ˆæ•°æ®æ¸…æ´— & æŒ‡æ ‡åŸºçº¿ï¼‰

* [ ] å»ºç¯å¢ƒï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶ `pip install -r requirements.txt`
* [ ] é…ç½® `configs/config.yaml`ï¼ˆå¸‚åœºã€æ—¥æœŸã€å‚æ•°ï¼‰
* [ ] `src/data/make_prices.py`ï¼šæ‹‰å–è¡Œæƒ…ï¼ˆé»˜è®¤ yfinanceï¼‰ï¼Œä¿å­˜ `data/raw/prices.parquet`
* [ ] `src/lib/metrics.py`ï¼šå®ç°å¹´åŒ–æ”¶ç›Šã€æ³¢åŠ¨ç‡ã€Sharpe/Sortinoã€æœ€å¤§å›æ’¤
* [ ] `src/data/prep_features.py`ï¼šå¯¹æ•°æ”¶ç›Šã€æ»šåŠ¨å‡å€¼/æ–¹å·®ã€z-scoreï¼ˆå…ˆæœ€å°ç‰¹å¾ï¼‰
* [ ] è¿è¡Œ `make data features`ï¼Œäº§å‡º `data/clean/features.parquet`
* [ ] å†™ `README.md` çš„â€œæ•°æ®å£å¾„&ç¼ºå¤±å¤„ç†â€å°èŠ‚

## Week 2ï¼ˆæ—¶åºCV & æœ€å°å›æµ‹å¯å¤ç°ï¼‰

* [ ] `src/lib/ts_cv.py`ï¼šå®ç° **Purged KFold + Embargo** çš„æ—¶åºäº¤å‰éªŒè¯
* [ ] `src/backtest/run_backtest.py`ï¼šLR åŸºçº¿ï¼ˆæ–¹å‘/è¶…é¢æ”¶ç›Šï¼‰ï¼Œå«äº¤æ˜“æˆæœ¬/æ»‘ç‚¹
* [ ] æ‰“é€š **MLflow**ï¼ˆè®°å½•å‚æ•°/æŒ‡æ ‡/å·¥ä»¶ï¼‰ä¸ **DVC**ï¼ˆæ•°æ®å¿«ç…§ï¼‰
* [ ] è·‘ `dvc repro` å®Œæˆ `fetch_data â†’ features â†’ backtest` å…¨é“¾æ¡
* [ ] ç”Ÿæˆé¦–ä»½æŠ¥å‘Šï¼š`reports/tables/metrics.json` ä¸ `reports/figures/equity_curve.png`
* [ ] å†™é—®é¢˜æ¸…å• & ä¸‹ä¸€æ­¥éœ€æ±‚ï¼ˆå¦‚ï¼šè¡Œä¸šä¸­æ€§ã€æ›´å¤šç‰¹å¾ã€ç¨³å¥æ€§è¯„ä¼°ï¼‰

---

# ğŸ“ é¡¹ç›®ç›®å½•éª¨æ¶

```text
quant-6mo/
â”œâ”€ Makefile
â”œâ”€ MLproject
â”œâ”€ dvc.yaml
â”œâ”€ .dvcignore
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ README.md
â”œâ”€ configs/
â”‚  â””â”€ config.yaml
â”œâ”€ data/
â”‚  â”œâ”€ raw/.gitkeep
â”‚  â”œâ”€ clean/.gitkeep
â”‚  â””â”€ interim/.gitkeep
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_cleaning.ipynb   # å¯ç©ºç™½å ä½ï¼Œç”¨äºæ¢ç´¢
â”‚  â””â”€ 02_ts_cv_demo.ipynb
â”œâ”€ reports/
â”‚  â”œâ”€ figures/.gitkeep
â”‚  â””â”€ tables/.gitkeep
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â””â”€ io.py
â”‚  â”œâ”€ lib/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ metrics.py
â”‚  â”‚  â””â”€ ts_cv.py
â”‚  â”œâ”€ data/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ make_prices.py
â”‚  â”‚  â””â”€ prep_features.py
â”‚  â”œâ”€ backtest/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ run_backtest.py
â”‚  â”‚  â””â”€ reporting.py
â”‚  â””â”€ service/
â”‚     â”œâ”€ __init__.py
â”‚     â””â”€ predictor.py
â””â”€ tests/
   â”œâ”€ test_metrics.py
   â””â”€ test_ts_cv.py
```

---

# ğŸ§° Makefileï¼ˆå¯ç›´æ¥ç”¨ï¼‰

```makefile
# ===== Makefile =====
PY=python
VENVDIR=.venv
PIP=$(VENVDIR)/bin/pip
PYBIN=$(VENVDIR)/bin/python
MLFLOW=$(VENVDIR)/bin/mlflow
DVC=$(VENVDIR)/bin/dvc

.PHONY: help setup clean data features backtest repro mlflow ui test fmt

help:
	@echo "Targets: setup | data | features | backtest | repro | mlflow | ui | test | fmt | clean"

setup:
	python -m venv $(VENVDIR)
	$(PIP) install --upgrade pip
	$(PIP) install -r requirements.txt
	@echo "âœ… setup done. Activate: source $(VENVDIR)/bin/activate"

data:
	$(PYBIN) -m src.data.make_prices --config configs/config.yaml

features:
	$(PYBIN) -m src.data.prep_features --config configs/config.yaml

backtest:
	$(PYBIN) -m src.backtest.run_backtest --config configs/config.yaml

repro:
	$(DVC) repro

mlflow:
	$(MLFLOW) ui --port 5000 --host 0.0.0.0

ui: mlflow

test:
	$(PYBIN) -m pytest -q

fmt:
	$(VENVDIR)/bin/isort src tests
	$(VENVDIR)/bin/black src tests

clean:
	rm -rf $(VENVDIR) .pytest_cache mlruns
	find . -name "*.pyc" -delete
```

---

# ğŸ“¦ requirements.txtï¼ˆç²¾ç®€å¯è·‘ï¼‰

```txt
pandas>=2.0
numpy>=1.26
scikit-learn>=1.3
statsmodels>=0.14
yfinance>=0.2
pyarrow>=14
mlflow>=2.12
dvc>=3.0
pyyaml>=6.0
matplotlib>=3.8
plotly>=5.20
numba>=0.59
pytest>=8.0
python-dotenv>=1.0
```

---

# ğŸ”§ MLprojectï¼ˆæœ€å°å¯å¤ç°å…¥å£ï¼‰

```yaml
name: quant-6mo

entry_points:
  backtest:
    parameters:
      config: {type: string, default: configs/config.yaml}
    command: "python -m src.backtest.run_backtest --config {config}"
  fetch:
    parameters:
      config: {type: string, default: configs/config.yaml}
    command: "python -m src.data.make_prices --config {config}"
```

> æ³¨ï¼šMLflow ä¸å¼ºåˆ¶éœ€è¦è¯¥æ–‡ä»¶ï¼Œä½†ä¿ç•™å¯ç›´æ¥ `mlflow run . -e backtest`ã€‚

---

# ğŸ—ƒ dvc.yamlï¼ˆæ•°æ®â†’ç‰¹å¾â†’å›æµ‹ ä¸‰é˜¶æ®µï¼‰

```yaml
stages:
  fetch_data:
    cmd: python -m src.data.make_prices --config configs/config.yaml
    deps:
      - src/data/make_prices.py
      - configs/config.yaml
    outs:
      - data/raw/prices.parquet

  features:
    cmd: python -m src.data.prep_features --config configs/config.yaml
    deps:
      - src/data/prep_features.py
      - data/raw/prices.parquet
      - configs/config.yaml
    outs:
      - data/clean/features.parquet

  backtest:
    cmd: python -m src.backtest.run_backtest --config configs/config.yaml
    deps:
      - src/backtest/run_backtest.py
      - src/lib/metrics.py
      - src/lib/ts_cv.py
      - data/clean/features.parquet
      - configs/config.yaml
    outs:
      - reports/tables/metrics.json
      - reports/figures/equity_curve.png
```

`.dvcignore`

```txt
mlruns/
.venv/
notebooks/
reports/figures/*.png
```

`.gitignore`

```txt
.venv/
__pycache__/
*.pyc
mlruns/
.DS_Store
.env
```

---

# âš™ï¸ configs/config.yamlï¼ˆé»˜è®¤å¯è·‘ï¼‰

```yaml
market:
  tickers: ["SPY", "QQQ"]
  start: "2015-01-01"
  end: "2025-01-01"
  interval: "1d"

features:
  windows: [5, 10, 20]
  target: "next_ret"   # ä¹Ÿå¯ "direction"

cv:
  n_splits: 5
  embargo_days: 5

trading:
  cost_bps: 5          # å•è¾¹ 0.05%
  slip_bps: 5
  max_weight: 1.0

seed: 42
paths:
  raw_prices: "data/raw/prices.parquet"
  features: "data/clean/features.parquet"
  equity_png: "reports/figures/equity_curve.png"
  metrics_json: "reports/tables/metrics.json"
```

---

# ğŸ§© src/utils/io.py

```python
from __future__ import annotations
import pandas as pd
from pathlib import Path


def ensure_parent(path: str | Path) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)


def save_parquet(df: pd.DataFrame, path: str | Path) -> None:
    ensure_parent(path)
    df.to_parquet(path, index=True)


def load_parquet(path: str | Path) -> pd.DataFrame:
    return pd.read_parquet(path)
```

---

# ğŸ“ˆ src/lib/metrics.pyï¼ˆåŸºç¡€æŒ‡æ ‡ï¼‰

```python
from __future__ import annotations
import numpy as np
import pandas as pd

TRADING_DAYS = 252


def annualized_return(returns: pd.Series) -> float:
    # æ—¥åº¦ç®€å•æ”¶ç›Šåºåˆ— â†’ å¹´åŒ–æ”¶ç›Š
    ret = (1 + returns).prod() ** (TRADING_DAYS / len(returns)) - 1
    return float(ret)


def annualized_vol(returns: pd.Series) -> float:
    return float(returns.std(ddof=0) * np.sqrt(TRADING_DAYS))


def sharpe(returns: pd.Series, rf: float = 0.0) -> float:
    ex = returns - rf / TRADING_DAYS
    vol = returns.std(ddof=0)
    return float(np.sqrt(TRADING_DAYS) * (ex.mean() / (vol + 1e-12)))


def sortino(returns: pd.Series, rf: float = 0.0) -> float:
    ex = returns - rf / TRADING_DAYS
    downside = ex[ex < 0].std(ddof=0)
    return float(np.sqrt(TRADING_DAYS) * (ex.mean() / (downside + 1e-12)))


def max_drawdown(returns: pd.Series) -> float:
    equity = (1 + returns).cumprod()
    peak = equity.cummax()
    dd = equity / peak - 1.0
    return float(dd.min())
```

---

# â± src/lib/ts\_cv.pyï¼ˆPurged KFold + Embargoï¼‰

```python
from __future__ import annotations
import numpy as np
import pandas as pd
from typing import Iterator, Tuple

class PurgedKFold:
    """Time-series split with purge and embargo.

    Splits by *unique dates* in chronological order. For each fold, removes
    an embargo window around the test block from the training set.
    """

    def __init__(self, n_splits: int = 5, embargo_days: int = 0):
        assert n_splits >= 2
        self.n_splits = n_splits
        self.embargo_days = embargo_days

    def split(self, X: pd.DataFrame) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        # Expect DatetimeIndex; if MultiIndex, use level 0.
        if isinstance(X.index, pd.MultiIndex):
            dates = X.index.get_level_values(0).normalize()
        else:
            dates = pd.to_datetime(X.index).normalize()
        uniq = dates.unique().sort_values()
        folds = np.array_split(uniq, self.n_splits)

        # Build map date -> row indices
        by_date = {}
        for d in uniq:
            by_date[d] = np.where(dates == d)[0]

        for i in range(self.n_splits):
            test_dates = pd.DatetimeIndex(folds[i])
            test_idx = np.concatenate([by_date[d] for d in test_dates])

            # Embargo range
            if self.embargo_days > 0:
                left = test_dates.min() - pd.Timedelta(days=self.embargo_days)
                right = test_dates.max() + pd.Timedelta(days=self.embargo_days)
                mask = (dates < left) | (dates > right)
            else:
                mask = ~dates.isin(test_dates)

            train_idx = np.where(mask)[0]
            yield train_idx, test_idx
```

---

# ğŸ§ª tests/test\_metrics.pyï¼ˆæœ€å°å•æµ‹ï¼‰

```python
import pandas as pd
from src.lib.metrics import annualized_return, max_drawdown

def test_basic_metrics():
    # æ„é€ ä¸€ä¸ªç¨³æ­¥ä¸Šå‡çš„ 100 å¤©æ”¶ç›Šåºåˆ—
    ret = pd.Series([0.001] * 100)
    assert annualized_return(ret) > 0
    assert max_drawdown(ret) == 0.0
```

# ğŸ§ª tests/test\_ts\_cv.py

```python
import pandas as pd
from src.lib.ts_cv import PurgedKFold

def test_purged_kfold_basic():
    idx = pd.date_range("2020-01-01", periods=100, freq="D")
    X = pd.DataFrame(index=idx, data={"x": range(100)})
    cv = PurgedKFold(n_splits=5, embargo_days=3)
    splits = list(cv.split(X))
    assert len(splits) == 5
    # ç¡®è®¤ train/test æ— é‡å 
    for tr, te in splits:
        assert set(tr).isdisjoint(set(te))
```

---

# ğŸ§® src/data/make\_prices.pyï¼ˆæ‹‰å–ä»·æ ¼å¹¶å­˜ Parquetï¼‰

```python
from __future__ import annotations
import argparse
import yaml
import yfinance as yf
import pandas as pd
from src.utils.io import save_parquet


def fetch_prices(tickers: list[str], start: str, end: str, interval: str) -> pd.DataFrame:
    df = yf.download(tickers, start=start, end=end, interval=interval, auto_adjust=True, progress=False)
    # yfinance å¤šæ ‡çš„åˆ—æ˜¯ MultiIndex: (field, ticker)
    if isinstance(df.columns, pd.MultiIndex):
        df = df.stack(level=1).rename_axis(index=["date", "ticker"]).reset_index()
    else:
        df["ticker"] = tickers[0]
        df = df.reset_index().rename(columns={"index": "date"})
    df = df.rename(columns={"Adj Close": "adj_close", "Close": "close"})
    df = df.set_index(["date", "ticker"]).sort_index()
    return df[["open", "high", "low", "close", "adj_close", "volume"]]


def main(cfg_path: str):
    with open(cfg_path, "r") as f:
        cfg = yaml.safe_load(f)
    m = cfg["market"]
    out = cfg["paths"]["raw_prices"]
    df = fetch_prices(m["tickers"], m["start"], m["end"], m.get("interval", "1d"))
    save_parquet(df, out)
    print(f"âœ… saved raw prices â†’ {out} rows={len(df):,}")


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()
    main(args.config)
```

---

# ğŸ§ª src/data/prep\_features.pyï¼ˆæœ€å°ç‰¹å¾ & ç›®æ ‡ï¼‰

```python
from __future__ import annotations
import argparse
import yaml
import numpy as np
import pandas as pd
from src.utils.io import load_parquet, save_parquet


def add_features(df: pd.DataFrame, windows: list[int]) -> pd.DataFrame:
    df = df.copy()
    df["ret"] = df.groupby(level=1)["adj_close"].pct_change().fillna(0.0)
    for w in windows:
        df[f"ret_mean_{w}"] = df.groupby(level=1)["ret"].rolling(w).mean().reset_index(level=0, drop=True)
        df[f"ret_std_{w}"] = df.groupby(level=1)["ret"].rolling(w).std().reset_index(level=0, drop=True)
        df[f"z_{w}"] = df[f"ret_mean_{w}"] / (df[f"ret_std_{w}"] + 1e-12)
    # ç›®æ ‡ï¼šä¸‹ä¸€æœŸæ”¶ç›Š & æ–¹å‘
    df["next_ret"] = df.groupby(level=1)["ret"].shift(-1)
    df["direction"] = (df["next_ret"] > 0).astype(int)
    return df.dropna()


def main(cfg_path: str):
    import warnings
    warnings.filterwarnings("ignore")
    cfg = yaml.safe_load(open(cfg_path))
    df = load_parquet(cfg["paths"]["raw_prices"])  # MultiIndex(date, ticker)
    feat = add_features(df, cfg["features"]["windows"])
    save_parquet(feat, cfg["paths"]["features"])
    print(f"âœ… saved features â†’ {cfg['paths']['features']} rows={len(feat):,}")


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()
    main(args.config)
```

---

# ğŸ” src/backtest/run\_backtest.pyï¼ˆæœ€å°å›æµ‹ + MLflowï¼‰

```python
from __future__ import annotations
import argparse, json, yaml
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import mlflow
from src.utils.io import load_parquet
from src.lib.ts_cv import PurgedKFold
from src.lib.metrics import annualized_return, annualized_vol, sharpe, sortino, max_drawdown


def equity_from_positions(returns: pd.Series, pos: pd.Series, cost_bps: float, slip_bps: float) -> pd.Series:
    # æˆæœ¬æŒ‰æ¢æ‰‹è®¡
    pos = pos.reindex(returns.index).fillna(0.0)
    churn = pos.diff().abs().fillna(0.0)
    tc = (cost_bps + slip_bps) / 1e4
    strat_ret = pos.shift(1).fillna(0.0) * returns - churn * tc
    return strat_ret


def run(cfg: dict):
    feat_path = cfg["paths"]["features"]
    feat = load_parquet(feat_path).reset_index()
    # åªç”¨ä»·æ ¼æ´¾ç”Ÿç‰¹å¾
    feature_cols = [c for c in feat.columns if c.startswith("ret_mean_") or c.startswith("ret_std_") or c.startswith("z_")]
    target = cfg["features"]["target"]

    # æŒ‰æ—¥æœŸèšåˆåˆ°æ—¥é¢‘ï¼ˆå¤š ticker ç®€åŒ–ä¸ºç­‰æƒï¼‰
    feat["date"] = pd.to_datetime(feat["date"])
    daily = feat.groupby("date").agg({**{c: "mean" for c in feature_cols}, **{target: "mean"}}).sort_index()

    # äº¤å‰éªŒè¯
    X = daily[feature_cols]
    y = (daily[target] > 0).astype(int)  # æ–¹å‘åˆ†ç±»

    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(C=1.0, max_iter=1000, n_jobs=None))
    ])

    cv = PurgedKFold(n_splits=cfg["cv"]["n_splits"], embargo_days=cfg["cv"]["embargo_days"])

    preds = pd.Series(index=daily.index, dtype=float)
    for tr_idx, te_idx in cv.split(X):
        pipe.fit(X.iloc[tr_idx], y.iloc[tr_idx])
        p = pipe.predict_proba(X.iloc[te_idx])[:, 1]
        preds.iloc[te_idx] = p

    # ç”Ÿæˆä»“ä½ï¼šæ¦‚ç‡>0.5 åšå¤šï¼Œå¦åˆ™ç©ºä»“ï¼ˆæœ€å°ç­–ç•¥ï¼‰
    pos = (preds > 0.5).astype(float).clip(0, cfg["trading"]["max_weight"])  # [0,1]

    # ç­–ç•¥æ”¶ç›Šï¼ˆç”¨æ—¥å‡ next_ret ä½œä¸ºè¿‘ä¼¼ï¼›ä¸¥æ ¼åº”é€ç¥¨èšåˆï¼‰
    rets = daily[cfg["features"]["target"]].rename("ret")
    strat_ret = equity_from_positions(rets, pos, cfg["trading"]["cost_bps"], cfg["trading"]["slip_bps"]).dropna()

    metrics = {
        "ann_ret": annualized_return(strat_ret),
        "ann_vol": annualized_vol(strat_ret),
        "sharpe": sharpe(strat_ret),
        "sortino": sortino(strat_ret),
        "max_dd": max_drawdown(strat_ret),
        "n_days": int(len(strat_ret)),
    }

    # ä¿å­˜å›¾è¡¨
    eq = (1 + strat_ret).cumprod()
    out_png = Path(cfg["paths"]["equity_png"])
    out_png.parent.mkdir(parents=True, exist_ok=True)
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10,4))
    eq.plot()
    plt.title("Equity Curve (baseline)")
    plt.tight_layout()
    plt.savefig(out_png)
    plt.close()

    # ä¿å­˜æŒ‡æ ‡
    out_json = Path(cfg["paths"]["metrics_json"])
    out_json.parent.mkdir(parents=True, exist_ok=True)
    out_json.write_text(json.dumps(metrics, indent=2))

    return metrics


def main(cfg_path: str):
    cfg = yaml.safe_load(open(cfg_path))
    mlflow.set_experiment("quant-6mo-baseline")
    with mlflow.start_run():
        mlflow.log_params({
            "tickers": ",".join(cfg["market"]["tickers"]),
            "windows": ",".join(map(str, cfg["features"]["windows"])),
            "n_splits": cfg["cv"]["n_splits"],
            "embargo_days": cfg["cv"]["embargo_days"],
            "cost_bps": cfg["trading"]["cost_bps"],
            "slip_bps": cfg["trading"]["slip_bps"],
        })
        metrics = run(cfg)
        mlflow.log_metrics(metrics)
        mlflow.log_artifact(cfg["paths"]["equity_png"])
        mlflow.log_artifact(cfg["paths"]["metrics_json"])
    print("âœ… backtest done:", metrics)


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()
    main(args.config)
```

---

# ğŸ“Š src/backtest/reporting.pyï¼ˆå¯é€‰ï¼šæ‰©å±•å›¾è¡¨ï¼‰

```python
from __future__ import annotations
import pandas as pd
import matplotlib.pyplot as plt

# é¢„ç•™ï¼šå åŠ å›æ’¤ã€åˆ† Regime è¡¨ç°ç­‰
```

---

# ğŸ§ª src/service/predictor.pyï¼ˆå ä½ï¼ŒWeek10 ç”¨ï¼‰

```python
class Predictor:
    def __init__(self, model=None):
        self.model = model
    def predict(self, X):
        return self.model.predict_proba(X)[:,1]
```

---

# ğŸ—’ README.mdï¼ˆæ ¸å¿ƒä½¿ç”¨è¯´æ˜ï¼‰

````markdown
# quant-6mo

## Quickstart
```bash
make setup
make data
make features
make backtest
````

## DVC Pipeline

```bash
dvc init
dvc repro
```

## MLflow UI

```bash
make mlflow  # http://localhost:5000
```

## ç»“æ„è¯´æ˜

* data/: åŸå§‹ä¸æ¸…æ´—åæ•°æ®ï¼ˆParquetï¼‰
* configs/config.yaml: å¸‚åœºä¸å›æµ‹å‚æ•°
* src/: æ•°æ®å¤„ç†ã€ç‰¹å¾ã€å›æµ‹ã€è¯„ä¼°
* reports/: å›¾è¡¨ä¸æŒ‡æ ‡è¾“å‡º

```
```
